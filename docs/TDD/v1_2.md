
### **Summary of Architectural Changes (v1.1 to v1.2)**

* **Asynchronous Execution:** Replaced the simplistic FastAPI async model with a dedicated background task queue using **GCP Cloud Tasks** and a separate worker service. This ensures reliability for long-running jobs.
* **State Management:** Introduced **Firestore** for persistent state management. The `GraphState` is now saved after each critical step, allowing jobs to be resumable and providing accurate status tracking.
* **Validator Optimization:** Re-architected the `truth_validator_node` to perform a single, batched LLM call against aggregated search results, drastically reducing latency and cost.
* **Video Assembly:** Replaced `moviepy` with direct `FFmpeg` command-line execution for better performance and scalability. The `Dockerfile` will include the FFmpeg binary.
* **Scoped Features:** Clarified that "animation" is out of scope for the initial version, re-branding the feature as "whiteboard-style images" to manage expectations.
* **Caching:** Added a **Memorystore (Redis)** caching layer for expensive API calls to reduce costs and improve performance on repeated requests.
* **Fault Tolerance:** Defined a specific retry and failure policy for each type of node, distinguishing between critical and non-critical steps.
* **Observability:** Added a detailed observability strategy including structured logging (with `structlog`), monitoring (Cloud Monitoring), and distributed tracing (OpenTelemetry).
* **Synchronization:** Specified the use of word-level timestamps from the ElevenLabs API for precise visual-to-narration synchronization.

---
Here is the fully updated technical documentation.

***

## **Project Technical Documentation: CognitoVid**

**Version:** 1.2
**Date:** August 5, 2025
**Author:** Gemini, Project Developer

### 1. Project Vision & Executive Summary

**CognitoVid** is an AI-driven application designed to automatically generate short, simple, and informative videos to explain complex terms or concepts. Given a user prompt, the system will produce a complete video asset, including a narrated script and corresponding visuals, with built-in safety features and extensive user customization.

This document outlines a robust and scalable architecture designed for production. It leverages a decoupled, queue-based system for handling long-running generation tasks, persistent state management for reliability, and a comprehensive observability stack for operational health. The system is architected to be efficient, resilient, and maintainable on the Google Cloud Platform (GCP).

### 2. Core Features

* **Automated Script Generation:** Creates a clear, concise, and easy-to-understand script based on the user's query and specified target audience.
* **Robust Content Moderation & Safety:** A multi-layered safety system that filters user inputs and AI-generated outputs.
* **Factual Validation:** An integrated workflow to cross-reference generated information against external knowledge sources.
* **Dynamic Visual Asset Creation:** Generates a sequence of relevant static images for key points in the script.
* **High-Quality Voiceover Synthesis:** Utilizes a leading text-to-speech (TTS) service with precise word-level timestamping for accurate synchronization.
* **End-to-End Automation:** A single API call triggers a resilient, asynchronous pipeline.
* **Rich User-Driven Customization:** Allows users to tailor the output by selecting parameters such as:
    * **Target Audience:** `child`, `teenager`, `university_student`, `expert`.
    * **Visual Style:** `photorealistic`, `cartoon`, `whiteboard_style_image`, `infographic`. (Note: Animation is out of scope for v1.2).
    * **Video Duration:** A `target_duration` in seconds (max 180s).
    * **Background Music:** A `music_style` such as `inspirational`, `calm`, or `upbeat`.
    * **Branding:** An optional `branding_logo_url` to watermark the video.
    * **Language:** Support for multiple languages (e.g., `en-US`, `es-ES`).

### 3. Technical Architecture & Stack

The architecture is designed around a decoupled, asynchronous model to ensure scalability and reliability.

#### 3.1. Technology Stack

* **Orchestration:** LangChain & LangGraph.
* **Data Modeling:** Pydantic.
* **Web API:** FastAPI (for accepting jobs).
* **Background Task Queue:** **GCP Cloud Tasks** to manage asynchronous jobs.
* **Worker Service:** Cloud Run instance that consumes tasks from the queue.
* **Persistent State Management:** **GCP Firestore** to store the state of each generation job.
* **Caching:** **GCP Memorystore (Redis)** for caching expensive API call results.
* **LLM (Script & Validation):** Google Cloud's Gemini Pro models.
* **Image Generation:** Google Cloud's Imagen 2.
* **Audio Generation:** ElevenLabs API (requesting word-level timestamps).
* **Video Assembly:** **FFmpeg** command-line tool (invoked via Python's `subprocess`).
* **Observability:** **OpenTelemetry** for tracing, `structlog` for structured logging, and **GCP Cloud Monitoring** for metrics and alerting.
* **Containerization:** Docker (with FFmpeg binary included).

#### 3.2. High-Level System Flow

1.  **Job Submission:** The user sends a POST request to the FastAPI `/generate` endpoint.
2.  **Job Creation:** The API validates the input, creates a new job document in **Firestore** with an initial "pending" state, and pushes a task containing the `job_id` to the **GCP Cloud Tasks** queue. It immediately returns the `job_id` to the user.
3.  **Task Execution:** A Cloud Run worker instance pulls the task from the queue. It reads the job details from the Firestore document using the `job_id`.
4.  **Graph Execution:** The worker executes the LangGraph pipeline. After each critical node, the worker updates the job's state in its Firestore document.
5.  **Status Check:** The user can poll a `/status/<job_id>` endpoint, which reads the latest state directly from the Firestore document without interrupting the worker.
6.  **Completion:** Upon completion, the worker updates the Firestore document with the final status ("completed" or "failed") and the URL to the final video.

### 4. The Core LangGraph Flow: The "Explainer Agent"

#### 4.1. Graph State & Persistence

The `GraphState` Pydantic model is persisted as a document in Firestore. A utility function `save_state_to_firestore(job_id, state)` is called after each node that makes a significant, non-idempotent change.

#### 4.2. Graph Nodes & Edges (Optimized)

1.  **`input_moderation_node`**: (As before) Checks initial prompt for safety.
2.  **`prompt_writer_node`**: (As before) Crafts the detailed meta-prompt.
3.  **`script_generator_node`**: (As before) Calls Gemini API, with results cached in Redis.
4.  **`truth_validator_node` (Optimized)**:
    * **Action:**
        1.  Performs a safety check on the `raw_script`.
        2.  Generates search queries for the main claims in the script.
        3.  Executes all search queries in parallel.
        4.  Makes a **single, batched call** to the Gemini API, providing the full script and all search results, requesting a single validation verdict and a list of corrections.
5.  **`scene_parser_node` (Enhanced)**:
    * **Action:** After generating scene descriptions and image prompts, this node aligns the script segments with the **word-level timestamps** received from the `audio_generator_node` to determine precise scene durations.
6.  **`image_generator_node`**: (As before) Calls Imagen API, with results cached in Redis.
7.  **`audio_generator_node`**:
    * **Action:** Calls the ElevenLabs API, **requesting word-level synchronization data** along with the audio file. Both are saved, and the state is updated.
8.  **`video_assembler_node` (Re-implemented)**:
    * **Action:** Does not use `moviepy`. It constructs and executes a series of **FFmpeg** commands via Python's `subprocess` module to:
        * Create video segments from each image, using the precise durations from the `scene_parser_node`.
        * Concatenate the image segments into a single silent video track.
        * Mix the narration audio track and the background music track.
        * Combine the final video and audio tracks.
        * Apply the branding watermark using FFmpeg's `overlay` filter.

### 5. Caching Strategy

A Redis cache (Memorystore) will be used to reduce costs and latency.
* **Cache Keys:** Keys will be generated by hashing the functional input of a node.
    * *Script Generation:* `hash(original_query + target_age + language + target_duration)`
    * *Image Generation:* `hash(image_prompt)`
    * *Audio Generation:* `hash(validated_script + language)`
* **Process:** Before executing an expensive API call, the node will check Redis for the key. On a cache miss, the API is called, and the result is stored in Redis with a Time-to-Live (TTL) of 24 hours.

### 6. Fault Tolerance and Retry Strategy

* **Idempotent Nodes:** Nodes that rely on cached data or read from Firestore are naturally idempotent.
* **API Call Nodes (`script`, `image`, `audio`):** Will be wrapped with a retry decorator (e.g., using the `tenacity` library) employing exponential backoff for transient errors (e.g., 5xx status codes).
* **Critical Failure:** Failure in `script_generator_node` or `audio_generator_node` after retries will mark the job as "failed" in Firestore.
* **Degradable Failure:** If a single image generation in `image_generator_node` fails after retries, the error is logged, a pre-defined placeholder image is used for that scene, and the graph continues execution. The final job status will be "completed_with_errors".

### 7. Observability Strategy

* **Structured Logging:** All services will use the `structlog` library to output JSON-formatted logs. Each log entry will automatically include the `job_id`, allowing for easy filtering and analysis in Google Cloud Logging.
* **Distributed Tracing:** **OpenTelemetry** will be integrated. Each major step in the pipeline (API call, LangGraph node execution) will be a "span" within a single trace tied to the `job_id`. This will provide a visual flame graph of the entire process, making it easy to identify performance bottlenecks.
* **Monitoring & Alerting:** A dashboard in Cloud Monitoring will be created to track key metrics, including:
    * **Business Metrics:** Number of jobs created/completed/failed per hour.
    * **Performance Metrics:** P95 latency for the entire job and for each critical node.
    * **Cost Metrics:** API costs per job (estimated via token counts).
    * **Error Metrics:** Rate of failed jobs, API error codes (4xx vs 5xx).
    * Alerts will be configured for high failure rates or significant increases in latency.

### 8. Application Development Phases

The phases are updated to reflect the new architecture.

* **Phase 1: Architectural Foundation:**
    * Set up the full GCP environment: Cloud Tasks, Firestore, Memorystore.
    * Create the FastAPI endpoint that creates a Firestore document and pushes to Cloud Tasks.
    * Create a basic Cloud Run worker that can consume tasks and update Firestore.
* **Phase 2: Core Logic & State Management:**
    * Build the LangGraph skeleton.
    * Implement persistent state management, saving the `GraphState` to Firestore after each mocked node.
    * Implement the Redis caching logic.
* **Phase 3: LLM, Audio, and Validation Integration:**
    * Integrate the Gemini and ElevenLabs APIs, including timestamp requests.
    * Implement the optimized `truth_validator_node`.
* **Phase 4: Visuals & FFmpeg Assembly:**
    * Integrate the Imagen API.
    * Re-implement the `video_assembler_node` using `FFmpeg` subprocess calls.
    * Create the `Dockerfile` including the FFmpeg binary.
* **Phase 5: Observability & Advanced Customization:**
    * Integrate OpenTelemetry, structured logging, and build the monitoring dashboard.
    * Implement music and branding customization features.
* **Phase 6: Internationalization & Beta:**
    * Implement full `language` support.
    * Prepare for a closed beta release.